# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset
df = pd.read_csv('uber.csv')


# 1. Pre-process the dataset
# Convert pickup and dropoff datetime to datetime format
df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])

# Extract features from datetime
df['pickup_hour'] = df['pickup_datetime'].dt.hour
df['pickup_day'] = df['pickup_datetime'].dt.day
df['pickup_month'] = df['pickup_datetime'].dt.month
df['pickup_dayofweek'] = df['pickup_datetime'].dt.dayofweek

# Drop irrelevant columns
df = df.drop(['pickup_datetime', 'key'], axis=1)

# Handle missing values (if any)
df = df.dropna()

# 2. Identify outliers using z-score
from scipy import stats
z_scores = np.abs(stats.zscore(df[['fare_amount', 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']]))
df = df[(z_scores < 3).all(axis=1)]

# 3. Check correlation
# plt.figure(figsize=(10, 6))
# sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
# plt.title('Correlation Matrix')
# plt.show()

# 4. Implement Linear Regression and Random Forest models
X = df.drop('fare_amount', axis=1)
y = df['fare_amount']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Linear Regression Model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)

# Random Forest Model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# 5. Evaluate the models
def evaluate_model(y_true, y_pred, model_name):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    print(f'{model_name} Performance:')
    print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')
    print(f'R-Squared (R2): {r2:.2f}\n')

# Evaluation for Linear Regression
evaluate_model(y_test, y_pred_lr, 'Linear Regression')

# Evaluation for Random Forest
evaluate_model(y_test, y_pred_rf, 'Random Forest')

# Plotting predicted vs actual values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_rf, label='Random Forest', alpha=0.5)
plt.scatter(y_test, y_pred_lr, label='Linear Regression', alpha=0.5)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2)
plt.legend()
plt.title('Actual vs Predicted Fare Amount')
plt.xlabel('Actual Fare Amount')
plt.ylabel('Predicted Fare Amount')
plt.show()



//2//
# Import required libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, confusion_matrix, accuracy_score
import numpy as np

# Load the dataset (replace with the path to your dataset if needed)
df = pd.read_csv('emails.csv')

# Split the dataset into features and labels
X = df.drop(columns=['Email No.', 'Prediction'])  # Features (drop non-relevant columns)
y = df['Prediction']  # Labels (0 - Not Spam, 1 - Spam)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize the feature set for both KNN and SVM
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

### K-NEAREST NEIGHBORS (KNN) ###

# Ask the user to input the value of k
k = int(input("Enter the value of k for K-Nearest Neighbors: "))

# Initialize KNN classifier with the user-defined k value
knn = KNeighborsClassifier(n_neighbors=k)

# Train the KNN model
knn.fit(X_train, y_train)

# Predict using KNN
y_pred_knn = knn.predict(X_test)

# Evaluate the KNN model
print("\nKNN Model Performance:")
print("KNN Accuracy:", accuracy_score(y_test, y_pred_knn))
print("KNN R²:", r2_score(y_test, y_pred_knn))
print("KNN MSE:", mean_squared_error(y_test, y_pred_knn))
print("KNN RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_knn)))
print("KNN MAE:", mean_absolute_error(y_test, y_pred_knn))
print("KNN Confusion Matrix:\n", confusion_matrix(y_test, y_pred_knn))


### SUPPORT VECTOR MACHINE (SVM) ###

# Initialize SVM classifier
svm = SVC(kernel='linear')

# Train the SVM model
svm.fit(X_train, y_train)

# Predict using SVM
y_pred_svm = svm.predict(X_test)

# Evaluate the SVM model
print("\nSVM Model Performance:")
print("SVM Accuracy:", accuracy_score(y_test, y_pred_svm))
print("SVM R²:", r2_score(y_test, y_pred_svm))
print("SVM MSE:", mean_squared_error(y_test, y_pred_svm))
print("SVM RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_svm)))
print("SVM MAE:", mean_absolute_error(y_test, y_pred_svm))
print("SVM Confusion Matrix:\n", confusion_matrix(y_test, y_pred_svm))


//3//

# Step 1: Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Step 2: Load the dataset
data = pd.read_csv('Churn_Modelling.csv')

# Drop irrelevant columns (RowNumber, CustomerId, and Surname)
data = data.drop(columns=['RowNumber', 'CustomerId', 'Surname'])

# Convert categorical variables to numerical values using one-hot encoding
data = pd.get_dummies(data, columns=['Geography', 'Gender'], drop_first=True)

# Define the features (X) and target variable (y)
X = data.drop(columns=['Exited'])
y = data['Exited']

# Split data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Normalize the data using StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 4: Build the neural network model
def build_model(activation_func):
    model = Sequential()
    # First hidden layer with 16 neurons and specified activation function
    model.add(Dense(16, activation=activation_func))
    # Second hidden layer with 8 neurons and specified activation function
    model.add(Dense(8, activation=activation_func))
    # Output layer with 1 neuron (for binary classification) and sigmoid activation
    model.add(Dense(1, activation='sigmoid'))

    # Compile the model with Adam optimizer and binary crossentropy loss
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Step 5: Train and evaluate the model with different activation functions
activations = ['relu', 'tanh', 'sigmoid']
results = {}

for activation in activations:
    print(f"Training model with {activation} activation function...")
    model = build_model(activation)

    # Train the model for 10 epochs with a batch size of 32
    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, validation_data=(X_test, y_test))

    # Make predictions on the test set
    y_pred = (model.predict(X_test) > 0.5).astype(int)

    # Evaluate the model's performance
    accuracy = accuracy_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)

    # Store the accuracy and confusion matrix for each activation function
    results[activation] = {
        "Accuracy": accuracy,
        "Confusion Matrix": cm
    }

# Step 6: Print the results for each activation function
for activation in activations:
    print(f"\nResults for {activation} activation function:")
    print(f"Accuracy: {results[activation]['Accuracy']:.4f}")
    print("Confusion Matrix:")
    print(results[activation]['Confusion Matrix'])
